# Awesome-Talking-Head-Generation

A curated list of papers focused on talking head animation, talking head animation, intend to keep pace with the anticipated surge of research in the coming months. 

# 知识星球

If you're interested in this research field, feel free to scan the QR code and join our community. The community **follows SOTA research every day** and offers a wealth of **technical blogs, reports**, and **learning materials**.

如果你对该研究方向感兴趣，欢迎扫码加入社群。社群**每日跟进前沿研究**，并有大量**技术博客、报告、及学习资料**。

<img src="https://github.com/NanGoAI/Awesome-Talking-Head-Generation/blob/main/docs/QR-Code.png" style="zoom:50%;" align="center" />

------



# Table-of-Contents

[2D-Audio-Driven](#2D-Audio-Driven)

[2D-Video-Driven](#2D-Video-Driven)

[3D-Audio-Driven](#3D-Audio-Driven)

[Dataset](#Dataset)

# 2D-Audio-Driven

## 2024

- DreamHead: Learning Spatial-Temporal Correspondence via Hierarchical Diffusion for Audio-driven Talking Head Synthesis; [arXiv 2024](https://arxiv.org/abs/2409.10281); 
- DiffTED: One-shot Audio-driven TED Talk Video Generation with Diffusion-based Co-speech Gestures; [CVPRW 2024](https://openaccess.thecvf.com/content/CVPR2024W/HuMoGen/papers/Hogue_DiffTED_One-shot_Audio-driven_TED_Talk_Video_Generation_with_Diffusion-based_Co-speech_CVPRW_2024_paper.pdf);
- EMOdiffhead: Continuously Emotional Control in Talking Head Generation via Diffusion; [arXiv 2024](https://arxiv.org/abs/2409.07255); 
- SVP: Style-Enhanced Vivid Portrait Talking Head Diffusion Model; [arXiv 2024](https://arxiv.org/abs/2409.03270); 
- PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for One-Shot Talking Head Generation; [arXiv 2024](https://arxiv.org/abs/2409.02657); [Project](https://junleen.github.io/projects/posetalk/); 
- SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing; [ACM MM 2024](https://arxiv.org/abs/2409.03605); 
- KAN-Based Fusion of Dual-Domain for Audio-Driven Facial Landmarks Generation; [arXiv 2024](https://www.arxiv.org/abs/2409.05330); 
- CyberHost: Taming Audio-driven Avatar Diffusion Model with Region Codebook Attention; [arXiv 2024](https://arxiv.org/abs/2409.01876); [Project](https://cyberhost.github.io/); 
- Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency; [arXiv 2024](https://arxiv.org/abs/2409.02634); [Project](https://loopyavatar.github.io/);
- FD2Talk: Towards Generalized Talking Head Generation with Facial Decoupled Diffusion Model; [ACM MM 2024](https://arxiv.org/abs/2408.09384); 
- EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions; [arXiv 2024](https://arxiv.org/abs/2407.08136); [Project](https://badtobest.github.io/echomimic.html); [Code](https://github.com/BadToBest/EchoMimic); 
- VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time; [Project](https://www.microsoft.com/en-us/research/project/vasa-1/); 
- AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animations; [arXiv 2024](https://arxiv.org/abs/2403.17694); [Code](https://github.com/Zejun-Yang/AniPortrait);
- Emote Portrait Alive: Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions; [ECCV 2024](https://arxiv.org/abs/2402.17485); [Project](https://humanaigc.github.io/emote-portrait-alive/); 
- Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation; [WACV 2024](https://arxiv.org/abs/2301.03396); [Project](https://mstypulkowski.github.io/diffusedheads/); [Code](https://github.com/MStypulkowski/diffused-heads); 
- GAIA: Zero-shot Talking Avatar Generation; [ICLR 2024](https://arxiv.org/abs/2311.15230);  [Project](https://gaiavatar.github.io/gaia/); 
- SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis; [CVPR 2024](https://arxiv.org/abs/2311.17590); [Project](https://ziqiaopeng.github.io/synctalk/); [Code](https://github.com/ZiqiaoPeng/SyncTalk); 

- VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior; [arXiv 2023](https://arxiv.org/abs/2312.01841); [Project](https://humanaigc.github.io/vivid-talk/); [Code](https://github.com/HumanAIGC/VividTalk); 
- MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions; [ICCV 2023](https://arxiv.org/abs/2307.10008); [Project](https://liuyunfei.net/projects/iccv23-moda/); 
- DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation; [CVPR 2023](https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_DiffTalk_Crafting_Diffusion_Models_for_Generalized_Audio-Driven_Portraits_Animation_CVPR_2023_paper.pdf); [Project](https://sstzal.github.io/DiffTalk/); [Code](https://github.com/sstzal/DiffTalk); 
- SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation; [CVPR 2023](https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_SadTalker_Learning_Realistic_3D_Motion_Coefficients_for_Stylized_Audio-Driven_Single_CVPR_2023_paper.html); [Project](https://sadtalker.github.io/); [Code](https://github.com/OpenTalker/SadTalker); 
- SPACE: Speech-driven Portrait Animation with Controllable Expression; [ICCV 2023](https://openaccess.thecvf.com/content/ICCV2023/papers/Gururani_SPACE_Speech-driven_Portrait_Animation_with_Controllable_Expression_ICCV_2023_paper.pdf); [Project](https://deepimagination.cc/SPACE/); 

- A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild; [ACM MM 2020](https://arxiv.org/abs/2008.10010); [Project](http://cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild/); [Code](https://github.com/Rudrabha/Wav2Lip); 
- MakeItTalk: Speaker-Aware Talking-Head Animation; [SIGGRAPH Asia 2020](https://arxiv.org/abs/2004.12992); [Project](https://people.umass.edu/~yangzhou/MakeItTalk/); [Code](https://github.com/yzhou359/MakeItTalk); 

# 2D-Video-Driven

- LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control; [arXiv 2024](https://arxiv.org/abs/2407.03168); [Project](https://liveportrait.github.io/); [Code](https://github.com/KwaiVGI/LivePortrait); 
- Learning Online Scale Transformation for Talking Head Video Generation; [arXiv 2024](https://arxiv.org/html/2407.09965v1); 
- EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars; [CVPR 2024](https://arxiv.org/abs/2404.19110); [Project](https://neeek2303.github.io/EMOPortraits/); [Code](https://github.com/neeek2303/EMOPortraits); 

- MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation; [CVPR 2023](https://arxiv.org/abs/2212.08062); [Project](https://meta-portrait.github.io/); [Code](https://github.com/Meta-Portrait/MetaPortrait); 

- MegaPortraits: One-shot Megapixel Neural Head Avatars; [ACM MM 2022](https://arxiv.org/abs/2207.07621); [Project](https://neeek2303.github.io/MegaPortraits/); 
- StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN; [ECCV 2022](https://arxiv.org/abs/2203.04036); [Project](https://feiiyin.github.io/StyleHEAT/); [Code](https://github.com/OpenTalker/StyleHEAT); 
- Thin-Plate Spline Motion Model for Image Animation; [CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_Thin-Plate_Spline_Motion_Model_for_Image_Animation_CVPR_2022_paper.pdf); [Code](https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model);
- Depth-Aware Generative Adversarial Network for Talking Head Video Generation; [CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Hong_Depth-Aware_Generative_Adversarial_Network_for_Talking_Head_Video_Generation_CVPR_2022_paper.pdf); [Project](https://harlanhong.github.io/publications/dagan.html); [Code](https://github.com/harlanhong/CVPR2022-DaGAN);

- PIRenderer: Controllable Portrait Image Generation via Semantic Neural Rendering; [ICCV 2021](https://openaccess.thecvf.com/content/ICCV2021/papers/Ren_PIRenderer_Controllable_Portrait_Image_Generation_via_Semantic_Neural_Rendering_ICCV_2021_paper.pdf); [Project](https://renyurui.github.io/PIRender_web/); [Code](https://github.com/RenYurui/PIRender); 
- One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing; [CVPR 2021](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_One-Shot_Free-View_Neural_Talking-Head_Synthesis_for_Video_Conferencing_CVPR_2021_paper.pdf); [Project](https://nvlabs.github.io/face-vid2vid/); [Code](https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis); 

- First Order Motion Model for Image Animation; [NeurIPS 2019](https://papers.nips.cc/paper_files/paper/2019/hash/31c0b36aef265d9221af80872ceb62f9-Abstract.html); [Code](https://github.com/AliaksandrSiarohin/first-order-model); 

# 3D-Audio-Driven

- KMTalk: Speech-Driven 3D Facial Animation with Key Motion Embedding; [ECCV 2024](https://arxiv.org/abs/2409.01113); [Code](https://github.com/ffxzh/KMTalk); 
- UniTalker: Scaling up Audio-Driven 3D Facial Animation through A Unified Model; [ECCV 2024](https://arxiv.org/abs/2408.00762); [Project](https://x-niper.github.io/projects/UniTalker/); [Code](https://github.com/X-niper/UniTalker); 

- CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior; [CVPR 2023](https://arxiv.org/abs/2301.02379); [Project](https://doubiiu.github.io/projects/codetalker/); [Code](https://github.com/Doubiiu/CodeTalker); 

- FaceFormer: Speech-Driven 3D Facial Animation with Transformers; [CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Fan_FaceFormer_Speech-Driven_3D_Facial_Animation_With_Transformers_CVPR_2022_paper.pdf); [Project](https://evelynfan.github.io/audio2face/); [Code](https://github.com/EvelynFan/FaceFormer); 

- MeshTalk: 3D Face Animation From Speech Using Cross-Modality Disentanglement; [ICCV 2021](https://openaccess.thecvf.com/content/ICCV2021/papers/Richard_MeshTalk_3D_Face_Animation_From_Speech_Using_Cross-Modality_Disentanglement_ICCV_2021_paper.pdf); [Code](https://github.com/facebookresearch/meshtalk);

## Dataset

| Dataset     | Year | Description                                                  | Link                                                        |
| ----------- | ---- | ------------------------------------------------------------ | ----------------------------------------------------------- |
| FEED        | 2024 | **Multi-View** Facial **Extreme Emotions** Dataset           | [Download](https://github.com/neeek2303/FEED)               |
| CelebV-Text | 2023 | A large-scale, **high-quality**, and **diverse** facial **text-video** dataset | [Download](https://celebv-text.github.io/)                  |
| VFHQ        | 2022 | A **high-quality video face** dataset                        | [Download](https://liangbinxie.github.io/projects/vfhq/)    |
| HDTF        | 2021 | A large in-the-wild **high-resolution audio-visual** dataset | [Download](https://github.com/MRzzm/HDTF)                   |
| MEAD        | 2020 | **Multi-view Emotional Audio-visual** Dataset                | [Download](https://wywu.github.io/projects/MEAD/MEAD.html)  |
| VoxCeleb    | 2017 | A large scale **audio-visual** dataset                       | [Download](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/) |

